{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q17gmm7G6VZX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAf08EAe6WPg",
        "outputId": "afd693b6-9478-406c-ff95-895beb43a702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: ['en', 'ro'] data/dump\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "LANGS = [x.strip() for x in os.getenv(\"LANGS\", \"en,ro\").split(\",\") if x.strip()]\n",
        "OUT_DIR = os.getenv(\"OUT_DIR\", \"data/dump\").strip()\n",
        "\n",
        "print(\"params:\", LANGS, OUT_DIR)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHhXRZOf7aIx"
      },
      "source": [
        "## Get spacy models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H6u2TFB9Tl8u"
      },
      "outputs": [],
      "source": [
        "# for Turkish, we'd have to run a separate pipeline with a different spacy version\n",
        "#! pip install \"tr_core_news_lg @ https://huggingface.co/turkish-nlp-suite/tr_core_news_lg/resolve/main/tr_core_news_lg-1.0-py3-none-any.whl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGNUWOHg7mKz",
        "outputId": "655a3524-95ab-4418-ffb6-314d6c44446c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xx-sent-ud-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "downloading en: en_core_web_lg\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "downloading ro: ro_core_news_lg\n",
            "Collecting ro-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ro_core_news_lg-3.8.0/ro_core_news_lg-3.8.0-py3-none-any.whl (568.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.5/568.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ro-core-news-lg\n",
            "Successfully installed ro-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ro_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download xx_sent_ud_sm\n",
        "\n",
        "language_models = {\n",
        "    \"en\": \"en_core_web_lg\",\n",
        "    \"ro\": \"ro_core_news_lg\",\n",
        "    \"de\": \"de_core_news_lg\",\n",
        "    \"fr\": \"fr_core_news_lg\",\n",
        "    \"it\": \"it_core_news_lg\",\n",
        "    \"es\": \"es_core_news_lg\",\n",
        "    \"nl\": \"nl_core_news_lg\",\n",
        "    \"pt\": \"pt_core_news_lg\",\n",
        "    \"sv\": \"sv_core_news_lg\",\n",
        "    \"pl\": \"pl_core_news_lg\",\n",
        "    \"ru\": \"ru_core_news_lg\",\n",
        "    \"uk\": \"uk_core_news_lg\",\n",
        "    \"el\": \"el_core_news_lg\",\n",
        "    \"hr\": \"hr_core_news_lg\",\n",
        "    \"lt\": \"lt_core_news_lg\",\n",
        "    \"mk\": \"mk_core_news_lg\",\n",
        "    \"sl\": \"sl_core_news_lg\",\n",
        "    \"ca\": \"ca_core_news_lg\",\n",
        "    \"zh\": \"zh_core_web_lg\",\n",
        "}\n",
        "\n",
        "for lm in LANGS:\n",
        "    if lm in language_models:\n",
        "        print(f\"downloading {lm}: {language_models[lm]}\")\n",
        "        !python -m spacy download {language_models[lm]}\n",
        "    else:\n",
        "        print(f\"use blank\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J7nrCc7ebN"
      },
      "source": [
        "## Get the MultiplEYE json data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4L0uFaHSk-y",
        "outputId": "cc5dfd1e-b6dc-4a1d-b1cc-34b048166aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-30 03:42:36--  https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/930203766/ec269415-b6bd-4aaa-b90d-9ab5aded3f27?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-30T04%3A38%3A20Z&rscd=attachment%3B+filename%3Dlanguages_json_all.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-30T03%3A37%3A47Z&ske=2025-09-30T04%3A38%3A20Z&sks=b&skv=2018-11-09&sig=tmdiSKEkjlxqyA%2FgoxBD6rdchupOA%2FOXl045D%2FHC8%2F0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTIwNDA1NiwibmJmIjoxNzU5MjAzNzU2LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.6ZZm1r00WISyfTLna_OoZiBliRiWGoWCTWjyz9Kenhs&response-content-disposition=attachment%3B%20filename%3Dlanguages_json_all.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-09-30 03:42:37--  https://release-assets.githubusercontent.com/github-production-release-asset/930203766/ec269415-b6bd-4aaa-b90d-9ab5aded3f27?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-30T04%3A38%3A20Z&rscd=attachment%3B+filename%3Dlanguages_json_all.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-30T03%3A37%3A47Z&ske=2025-09-30T04%3A38%3A20Z&sks=b&skv=2018-11-09&sig=tmdiSKEkjlxqyA%2FgoxBD6rdchupOA%2FOXl045D%2FHC8%2F0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTIwNDA1NiwibmJmIjoxNzU5MjAzNzU2LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.6ZZm1r00WISyfTLna_OoZiBliRiWGoWCTWjyz9Kenhs&response-content-disposition=attachment%3B%20filename%3Dlanguages_json_all.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 562240 (549K) [application/octet-stream]\n",
            "Saving to: ‘languages_json_all.zip’\n",
            "\n",
            "languages_json_all. 100%[===================>] 549.06K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-30 03:42:37 (22.7 MB/s) - ‘languages_json_all.zip’ saved [562240/562240]\n",
            "\n",
            "Archive:  languages_json_all.zip\n",
            "   creating: languages_json/\n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ca.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_en.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_mk.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_pt.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_sl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_lt.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_es.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_lv.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_nl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_fr.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_rm.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_yue.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_pl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_el.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_kl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_sq.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_hi.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_eu.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_cs.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_et.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_zh.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ro.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ru.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ar.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_it.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_hr.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_zd.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_sv.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_uk.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_de.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_tr.json  \n"
          ]
        }
      ],
      "source": [
        "! rm -rf languages*\n",
        "! wget https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
        "! unzip languages_json_all.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0xc3i_xvQU1Z"
      },
      "outputs": [],
      "source": [
        "SPACY_LANGUAGES = [\"ca\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hr\", \"it\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"sl\", \"sv\", \"uk\", \"zh\"]\n",
        "\n",
        "CODE2LANG = {\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"cs\": \"Czech\",\n",
        "    \"de\": \"German\",\n",
        "    \"gsw\": \"Swiss German\",\n",
        "    \"el\": \"Greek\",\n",
        "    \"en\": \"English\",\n",
        "    #\"es\": \"Spanish\",\n",
        "    \"et\": \"Estonian\",\n",
        "    \"eu\": \"Basque\",\n",
        "    #\"fr\": \"French\",\n",
        "    #\"he\": \"Hebrew\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    \"hr\": \"Croatian\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"kl\": \"Kalaallisut\",\n",
        "    \"lt\": \"Lithuanian\",\n",
        "    \"lv\": \"Latvian\",\n",
        "    \"mk\": \"Macedonian\",\n",
        "    \"nl\": \"Dutch\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"rm\": \"Romansh\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"sl\": \"Slovenian\",\n",
        "    \"sq\": \"Albanian\",\n",
        "    \"sv\": \"Swedish\",\n",
        "    \"tr\": \"Turkish\",\n",
        "    \"uk\": \"Ukrainian\",\n",
        "    #\"yue\": \"Cantonese\",\n",
        "    \"zh\": \"Chinese\"\n",
        "}\n",
        "\n",
        "LANGUAGES = list(CODE2LANG.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDPp5lF77uB2"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R27JzBYj9iIw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def load_all_json(lang_folder):\n",
        "    all_data = {}\n",
        "    for file in os.listdir(lang_folder):\n",
        "        if file.endswith('.json'):\n",
        "            lang_code = file.replace('.json', '').replace('multipleye_stimuli_experiment_', '')\n",
        "            if lang_code == 'zd':\n",
        "                lang_code = 'gsw'\n",
        "            if (lang_code not in LANGUAGES) or (lang_code not in LANGS):\n",
        "                continue\n",
        "            with open(os.path.join(lang_folder, file), 'r', encoding='utf-8') as f:\n",
        "                all_data[lang_code] = json.load(f)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdZjNWw9i6C",
        "outputId": "fc1daa7c-47d0-4f60-bd13-d62f32d89601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ro {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['Proiectul MultiplEYE\\n\\nNumele „MultiplEYE” este un joc de cuvinte care combină „multilingvism” sau „limbi multiple” cu „eye” (ochi) din „eye-tracking” (urmărire oculară). MultiplEYE este o Acțiune COST finanțată de Uniunea Europeană. Acțiunile COST sunt rețele de cercetare sprijinite de Cooperarea Europeană în Știință și Tehnologie, pe scurt COST. Ca organizație de finanțare, COST susține rețeaua noastră în creștere de cercetători din Europa și din afara ei, oferind sprijin financiar pentru desfășurarea diverselor activități de networking.', 'Aceste activități includ întâlniri ale grupurilor de lucru, școli de formare pentru a împărtăși abilități cu cercetătorii mai tineri și vizite științifice de cercetare. Titlul proiectului Acțiunii COST MultiplEYE este: Facilitarea colectării datelor de urmărire oculară în mai multe limbi pentru cercetarea procesării limbajului de către om și a procesării automate. Acest lucru înseamnă că Acțiunea COST MultiplEYE urmărește să încurajeze o rețea interdisciplinară de grupuri de cercetare care lucrează la colectarea datelor de urmărire oculară în timpul citirii în mai multe limbi.', 'Obiectivul este de a sprijini dezvoltarea unui corpus multilingv extins de urmărire oculară și de a permite cercetătorilor să colecteze date prin împărtășirea cunoștințelor între diverse domenii, inclusiv lingvistică, psihologie, logopedie și informatică. Aceste date pot fi apoi utilizate pentru a studia procesarea limbajului uman dintr-o perspectivă psiholingvistică, precum și pentru a îmbunătăți și evalua procesarea computațională a limbajului dintr-o perspectivă de învățare automată.', 'Ce este urmărirea oculară?\\nUrmărirea oculară este procesul de măsurare a punctului de privire – locul unde te uiți – și a mișcărilor ochilor între punctele fixe de privire. Dispozitivul utilizat pentru a măsura pozițiile și mișcările ochilor se numește eye-tracker. Acesta constă într-o cameră cu infraroșu, folosind o frecvență de lumină care nu deranjează sau rănește ochiul uman.', 'Cu ajutorul algoritmilor de recunoaștere a imaginii, eye-tracker-ul poate estima punctele de privire cu mare precizie, cunoscând poziția capului și a ochilor, distanța față de ecranul la care se uită participantul și poziția dispozitivului. Urmărirea oculară este o tehnologie utilă pentru multe aplicații. De exemplu, poate ajuta la detectarea oboselii în timpul condusului sau poate sprijini aplicațiile pentru screening și instruire în domeniul medical. Urmărirea oculară este folosită, de asemenea, în gaming, marketing și interacțiunea om-calculator.', 'De ce urmărirea oculară în timpul citirii prezintă interes pentru proiectul nostru?\\nÎn timp ce citești aceste cuvinte, eye-tracker-ul urmărește mișcările ochilor tăi pe text. Acest lucru oferă informații despre cât timp petreci uitându-te la un text sau, mai specific, cât timp ai petrecut pe fiecare cuvânt, ce cuvinte ai sărit, asupra căror cuvinte te-ai oprit și dacă a trebuit să revii și să recitești părți ale textului pentru a-l înțelege mai bine.', 'Pe măsură ce creierul tău procesează conținutul textului, mișcările ochilor reflectă multe dintre procesele lingvistice și cognitive care au loc aproape în timp real. Astfel, datele înregistrate sunt o adevărată comoară de informații despre cum construim sensul și structurile gramaticale ale unui text. Ele arată cu care părți ale textului întâmpinăm dificultăți și care sunt ușor de citit. Este responsabilitatea cercetătorilor să explice ulterior ce factori lingvistici au cauzat diferitele tipuri de mișcări ale ochilor.', 'Motivația din spatele MultiplEYE este că datele de urmărire oculară sunt încă puține, mai ales pentru limbile cu un număr mai mic de vorbitori. O astfel de colectare extinsă a datelor reprezintă o provocare în ceea ce privește dezvoltarea și convenirea asupra designului experimental, complexității și tipurilor de texte care urmează să fie citite de către participanți. Alte decizii care par mai puțin relevante, dar sunt de fapt foarte importante, includ tipul și dimensiunea fontului în care este prezentat textul, ordinea textelor, procedura experimentului și modul în care vor fi procesate datele.', 'Dar, odată finalizat, acest set de date ne va permite să investigăm multe subiecte legate de psiholingvistică și lingvistică computațională. De exemplu, putem compara comportamentul de citire între diferite limbi. Influențează oare tipul de scriere, de exemplu, alfabetul latin față de scrierea chirilică sau arabă, timpii de citire? Un exemplu legat de procesarea computațională a textului ar putea implica utilizarea datelor de urmărire oculară pentru a îmbunătăți aplicațiile de inteligență artificială care imită procesul de citire uman. Acest lucru ar putea fi folosit pentru a construi sisteme mai bune de traducere automată sau pentru a îmbunătăți extragerea automată a cuvintelor cheie din text.', 'Primirea datelor de urmărire oculară de la mulți participanți, inclusiv de la tine, prin citirea textelor în multe limbi diferite, va constitui o bază excelentă pentru cercetarea noastră. Aceasta va fi factorul principal care va transforma rețeaua noastră de cercetare într-un demers de succes. Sperăm să deschidem calea pentru avansarea cercetării în diverse subdomenii ale lingvisticii prin sprijinirea și conectarea unui grup mare de cercetători.', 'Principalele rezultate ale Acțiunii MultiplEYE vor fi un set mare de date care conține date de urmărire oculară în multe limbi și o platformă pentru noi colaborări care se bazează pe acest tip de date. Dacă citești acest text, deja ne sprijini cauza, permițându-ne să colectăm și să analizăm mișcările ochilor tăi în timp ce citești și înțelegi limbajul. Îți mulțumim!']}\n",
            "en {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['The MultiplEYE Project\\n\\nThe name \"MultiplEYE\" is a wordplay combining \"multilingualism\" or \"multiple languages\" with \"eye\" from \"eye-tracking\". MultiplEYE is a COST Action funded by the European Union. COST Actions are research networks supported by the European Cooperation in Science and Technology or COST for short. As a funding organisation, COST supports our growing network of researchers across Europe and beyond by providing financial assistance for conducting different networking activities.', 'These activities include working group meetings, training schools to share skills with younger researchers, and scientific research visits. The project title of the MultiplEYE COST Action is: Enabling multilingual eye-tracking data collection for human and machine language processing research. This means that the MultiplEYE COST Action aims to foster an interdisciplinary network of research groups working on collecting eye-tracking data from reading in multiple languages.', 'The goal is to support the development of a large multilingual eye-tracking corpus and enable researchers to collect data by sharing their knowledge between various fields, including linguistics, psychology, speech and language pathology, and computer science. This data collection can then be used to study human language processing from a psycholinguistic perspective as well as to improve and evaluate computational language processing from a machine-learning perspective.', 'What is \"eye-tracking\"?\\nEye-tracking is the process of measuring the point of gaze - where you are looking - and the movements of the eyes between fixed points of gaze. The device used to measure the eye positions and eye movements is called an eye-tracker. It consists of an infrared camera, using a light frequency that does not bother or hurt the human eye.', \"With the help of image recognition algorithms, the eye-tracker can estimate gaze points very accurately by knowing the position of the head and eyes, the distance to the screen a participant is looking at and the eye-tracker's position. Eye-tracking is a helpful technology for many applications. For example, it can help detect tiredness while driving or it can support applications for screening and training purposes in the medical domain. Eye-tracking is also used in gaming, marketing, and human-computer interaction.\", \"Why is eye-tracking while reading especially interesting for our project?\\nAs you read these words, the eye-tracker follows your eye's movements over the text. This provides information about how long you spend looking at a text, or more specifically, how long you spent on each word, which words you skipped, which words you dwelled on, and whether you had to go back and reread parts of the text to understand it better.\", \"As your brain is processing the content of the text, your eye movements reflect a lot of the linguistic and cognitive processing going on almost in real time. Thus, the recorded data is a gold mine of information about how we put together a text's meaning and grammatical structures. It shows which parts of the text we struggle with and which parts are easily readable. It is up to the researchers to later explain which linguistic factors caused which type of eye movements.\", 'The motivation behind MultiplEYE is that eye-tracking data is still sparse, especially for languages with fewer speakers. Such extensive data collection is a challenge in terms of developing and agreeing on the experimental design, the complexity, and the types of texts to be read by the participants. Other decisions that seem less relevant but are, in fact, very important include the font type and size the text is presented in, the order of the texts, the experiment procedure, and how the data will be processed.', 'But once completed, this dataset will allow us to investigate many topics related to psycholinguistics and computational linguistics. For example, we can compare the reading behaviour across different languages. Does the scripture, for example, the Latin alphabet versus Cyrillic or Arabic scripts, impact reading times? An example concerning the computational processing of text could involve using eye-tracking data to advance artificial intelligence applications that imitate the human reading process. This could be used to build better machine translation systems or to improve the automatic extraction of keywords from the text.', 'Receiving eye-tracking data from many participants, including yourself, by reading texts in many different languages will be a great foundation for our research. It will be the main factor in turning our research network into a successful endeavour. We hope to clear the way for advancing research in various subfields of linguistics by supporting and connecting a large group of researchers.', 'The main outcomes of the MultiplEYE Action will be a large dataset containing eye-tracking data in many languages and a platform for new collaborations building on this type of data. If you are reading this text, you are already supporting our cause by allowing us to collect and analyse your eye movements while reading and comprehending language. Thank you!']}\n"
          ]
        }
      ],
      "source": [
        "all_data = load_all_json('languages_json')\n",
        "for k,v in all_data.items():\n",
        "  print(k, v[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Z0HnuEDdxt"
      },
      "source": [
        "## Prepare spaCy code to generate template csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Hnvn1bCFWLF_"
      },
      "outputs": [],
      "source": [
        "\n",
        "LANG_FOLDER = \"languages_json\"\n",
        "NLP_MODEL = None\n",
        "CURRENT_LANG = ''\n",
        "IN_DIR = 'languages_json/'\n",
        "\n",
        "from spacy.util import get_lang_class\n",
        "\n",
        "\n",
        "def exists_spacy_blank(lang_code):\n",
        "    try:\n",
        "        get_lang_class(lang_code)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def load_spacy_model(lang_code, small=True):\n",
        "    model = None\n",
        "    if lang_code in SPACY_LANGUAGES:\n",
        "        genre = 'news'\n",
        "        if lang_code in {'zh', 'en'}:\n",
        "            genre = 'web'\n",
        "        if lang_code == 'rm':\n",
        "            return ''\n",
        "        model_name = f'{lang_code}_core_{genre}_{\"sm\" if small else \"lg\"}'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    elif lang_code == \"rm\":\n",
        "        model = spacy.load(\"it_core_news_lg\")\n",
        "        # keep 'morphologizer' ?\n",
        "        model.disable_pipes('tok2vec', 'tagger', 'parser', 'lemmatizer', 'attribute_ruler', 'ner')\n",
        "    elif lang_code == 'gsw':\n",
        "        model = spacy.load('de_core_news_lg')\n",
        "    elif exists_spacy_blank(lang_code):\n",
        "        print(f\"Loading model blank model for {lang_code}\")\n",
        "        model = spacy.blank(lang_code)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    else:\n",
        "        model_name = f'xx_sent_ud_sm'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_nlp(lang_code, small=False):\n",
        "    \"\"\"To avoid loading all models at the same time\n",
        "    \"\"\"\n",
        "    global NLP_MODEL, CURRENT_LANG\n",
        "    if lang_code != CURRENT_LANG:\n",
        "        try:\n",
        "            print(f\"Deleting model for {CURRENT_LANG}\")\n",
        "            del NLP_MODEL\n",
        "        except:\n",
        "            print(\"No model to delete\")\n",
        "        print(f\"Loading model for {lang_code}\")\n",
        "        NLP_MODEL = load_spacy_model(lang_code, small=small)\n",
        "        CURRENT_LANG = lang_code\n",
        "    return NLP_MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "13jbiTKgbx4O"
      },
      "outputs": [],
      "source": [
        "def feats_str(token):\n",
        "    if not token.morph:\n",
        "        return \"_\"\n",
        "    md = token.morph.to_dict()\n",
        "    if not md:\n",
        "        return \"_\"\n",
        "    bits = []\n",
        "    for k in sorted(md):\n",
        "        v = md[k]\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            bits.append(f\"{k}={','.join(v)}\")\n",
        "        else:\n",
        "            bits.append(f\"{k}={v}\")\n",
        "    return \"|\".join(bits) if bits else \"_\"\n",
        "\n",
        "\n",
        "def get_head(token, sent):\n",
        "    if token.head == token or token.dep_ == \"ROOT\":\n",
        "        head = 0\n",
        "        deprel = \"root\"\n",
        "    else:\n",
        "        head = (token.head.i - sent.start) + 1  # 1-based in sentence\n",
        "        deprel = token.dep_.lower() if token.dep_ else \"_\"\n",
        "    return head, deprel\n",
        "\n",
        "\n",
        "def get_misc(token, include_ner=True):\n",
        "    misc_parts = []\n",
        "    if not token.whitespace_:\n",
        "        misc_parts.append(\"SpaceAfter=No\")\n",
        "    if include_ner and token.ent_iob_ != \"O\":\n",
        "        misc_parts.append(f\"NER={token.ent_iob_}-{token.ent_type_}\")\n",
        "    misc = \"|\".join(misc_parts) if misc_parts else \"_\"\n",
        "    return misc\n",
        "\n",
        "\n",
        "def iter_pages(stimuli, nlp):\n",
        "    for stim in stimuli:\n",
        "        sid, sname = stim[\"stimulus_id\"], stim[\"stimulus_name\"]\n",
        "        for pnum, page_text in enumerate(stim[\"pages\"], start=1):\n",
        "            yield sid, sname, pnum, nlp(page_text)\n",
        "\n",
        "def stimuli2csv(stimuli, lang_code, level=\"page\", small=False):\n",
        "    rows = []\n",
        "    nlp = get_nlp(lang_code, small=small)\n",
        "    for sid, sname, page, doc in iter_pages(stimuli, nlp):\n",
        "        ptext = doc.text\n",
        "        document = nlp(ptext)\n",
        "        for sent_idx, sentence in enumerate(document.sents):\n",
        "            eos = {\n",
        "              \"language\": CODE2LANG[lang_code],\n",
        "              \"language_code\": lang_code,\n",
        "              \"stimulus_name\": sname,\n",
        "              \"page\": page,\n",
        "              #\"sent_idx\": sent_idx+1,\n",
        "              \"token\": \"<eos>\",\n",
        "              \"is_alpha\": False,\n",
        "              \"is_stop\": False,\n",
        "              \"is_punct\": False,\n",
        "              \"lemma\": \"\",\n",
        "              \"upos\": \"\",\n",
        "              \"xpos\": \"\",\n",
        "              \"feats\": \"\",\n",
        "              \"head\": \"\",\n",
        "              \"deprel\": \"\",\n",
        "              \"deps\": \"\",\n",
        "              \"misc\": \"\"\n",
        "              }\n",
        "            for token in sentence:\n",
        "                head, deprel = get_head(token, sentence)\n",
        "                rows.append(\n",
        "                    {\n",
        "                        #\"stimulus_id\": sid,\n",
        "                        \"language\": CODE2LANG[lang_code],\n",
        "                        \"language_code\": lang_code,\n",
        "                        \"stimulus_name\": sname,\n",
        "                        \"page\": page,\n",
        "                        #\"sent_idx\": sent_idx+1,\n",
        "                        \"token\": token.text,\n",
        "                        \"is_alpha\": token.is_alpha,\n",
        "                        \"is_stop\": token.is_stop,\n",
        "                        \"is_punct\": token.is_punct,\n",
        "                        \"lemma\": token.lemma_,\n",
        "                        \"upos\": token.pos_,\n",
        "                        \"xpos\": token.tag_,\n",
        "                        \"feats\": feats_str(token),\n",
        "                        \"head\": head,\n",
        "                        \"deprel\": deprel,\n",
        "                        \"deps\": \"_\",\n",
        "                        \"misc\": get_misc(token, include_ner=True)\n",
        "                    }\n",
        "                )\n",
        "            rows.append(eos)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(by=[\"stimulus_name\", \"page\"])\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4-VqxpOrt_"
      },
      "source": [
        "## Generate csv templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMb69Drkbx7D",
        "outputId": "cf8badfe-82c9-4bfc-f179-bb8eff82daa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting model for \n",
            "Loading model for ro\n",
            "Loading model ro_core_news_lg for ro\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [00:08<00:08,  8.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting model for ro\n",
            "Loading model for en\n",
            "Loading model en_core_web_lg for en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:16<00:00,  8.43s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "preproc = defaultdict(dict)\n",
        "for lang_code, data in tqdm(all_data.items()):\n",
        "    if lang_code not in LANGS:\n",
        "        continue\n",
        "    preproc[lang_code] = stimuli2csv(data, lang_code, small=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkFLQik9U-sm"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBjlX2JFURsg",
        "outputId": "0cb46530-0188-4b42-a9ce-c5630fd42b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 13.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/dump/ro/Arg_PISACowsMilk.csv\n",
            "data/dump/ro/Arg_PISARapaNui.csv\n",
            "data/dump/ro/Enc_WikiMoon.csv\n",
            "data/dump/ro/Ins_HumanRights.csv\n",
            "data/dump/ro/Ins_LearningMobility.csv\n",
            "data/dump/ro/Lit_Alchemist.csv\n",
            "data/dump/ro/Lit_BrokenApril.csv\n",
            "data/dump/ro/Lit_MagicMountain.csv\n",
            "data/dump/ro/Lit_NorthWind.csv\n",
            "data/dump/ro/Lit_Solaris.csv\n",
            "data/dump/ro/PopSci_Caveman.csv\n",
            "data/dump/ro/PopSci_MultiplEYE.csv\n",
            "data/dump/en/Arg_PISACowsMilk.csv\n",
            "data/dump/en/Arg_PISARapaNui.csv\n",
            "data/dump/en/Enc_WikiMoon.csv\n",
            "data/dump/en/Ins_HumanRights.csv\n",
            "data/dump/en/Ins_LearningMobility.csv\n",
            "data/dump/en/Lit_Alchemist.csv\n",
            "data/dump/en/Lit_BrokenApril.csv\n",
            "data/dump/en/Lit_MagicMountain.csv\n",
            "data/dump/en/Lit_NorthWind.csv\n",
            "data/dump/en/Lit_Solaris.csv\n",
            "data/dump/en/PopSci_Caveman.csv\n",
            "data/dump/en/PopSci_MultiplEYE.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "for lang_code, df in tqdm(preproc.items()):\n",
        "    lang_out = 'gsw' if lang_code == 'zd' else lang_code\n",
        "    out_dir = os.path.join(OUT_DIR, lang_out)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    for stim_name, group in df.groupby('stimulus_name'):\n",
        "        out_fis = os.path.join(out_dir, f\"{stim_name}.csv\")\n",
        "        g = group.copy()\n",
        "        g['language_code'] = lang_out\n",
        "        g.to_csv(out_fis, index=False)\n",
        "        print(out_fis)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
